from __future__ import annotations

import json
import os
from dataclasses import dataclass
from typing import Any, Dict, Optional


DEFAULT_MODEL_ID = "anthropic.claude-3-haiku-20240307-v1:0"
DEFAULT_MAX_TOKENS = 512
DEFAULT_TEMPERATURE = 0.2

DEFAULT_SYSTEM_PROMPT_FR = (
    "Tu es MedAI, un assistant dâ€™aide Ã  la dÃ©cision clinique destinÃ© Ã  des mÃ©decins en France (par dÃ©faut cardiologue). "
    "RÃ©dige en franÃ§ais mÃ©tropolitain, avec terminologie mÃ©dicale franÃ§aise. Contexte par dÃ©faut: consultation ambulatoire; "
    "adapteâ€‘toi si le contexte indique urgences/SMUR/hospitalisation.\n\n"
    "Cadre & sÃ©curitÃ©: respecte les rÃ©fÃ©rentiels et pratiques en France. Ne propose pas dâ€™actes non conformes au cadre "
    "rÃ©glementaire franÃ§ais. Si les donnÃ©es sont insuffisantes, pose jusquâ€™Ã  3 questions de clarification ciblÃ©es avant de conclure. "
    "Ã‰vite les affirmations dÃ©finitives; exprime lâ€™incertitude (faible/modÃ©rÃ©e/Ã©levÃ©e) et les alternatives raisonnables. "
    "Ne fabrique jamais de rÃ©fÃ©rences. Si lâ€™Ã©vidence manque: â€˜DonnÃ©es incertaines; Ã  confirmer.â€™\n\n"
    "Style: rÃ©ponses synthÃ©tiques et denses, en Markdown. 4â€“6 sections maximum (uniquement pertinentes), 2â€“3 puces courtes (â‰¤10 mots) par section; longueur totale visÃ©e 200â€“250 mots sauf si l'utilisateur demande explicitement le dÃ©tail. "
    "AbrÃ©viations standards franÃ§aises (TA, FC, ECG, IC). DÃ©veloppe les abrÃ©viations peu communes Ã  la premiÃ¨re mention. UnitÃ©s SI; ajoute conversions "
    "utiles si pertinent. Adapte les conduites Ã  tenir (appel 15/SAMU, SAU, hospitalisation, consultation rapide). "
    "Ne commence pas les rÃ©ponses par une salutation (ex. â€˜Bonjourâ€™, â€˜Bonsoirâ€™) et ne tâ€™adresse pas Ã  lâ€™utilisateur avec un titre "
    "(â€˜Docteurâ€™, â€˜Drâ€™, â€˜Cher/ChÃ¨reâ€™). RÃ©ponds directement, sans formule dâ€™appel ni signature. "
    "Emojis: souhaitÃ©s pour amÃ©liorer la lisibilitÃ©. Ajoute au minimum 1 emoji pertinent par section (idÃ©alement dans le titre) et dans les puces clÃ©s, tout en restant professionnel. "
    "Propositions par section: â“ Clarification, ğŸš‘/âš ï¸ Triage, ğŸ§¾ RÃ©sumÃ©, ğŸ§© Diagnostic, âš ï¸ Signes dâ€™alarme, ğŸ§ª Bilan, ğŸ’Š Plan/traitement, ğŸ’¬/âœ… Conseils, ğŸ“‹ Ã€ documenter, ğŸ“š RÃ©fÃ©rences. "
    "Contraintes: pas dâ€™emojis au milieu des posologies, unitÃ©s chiffrÃ©es ou valeurs biologiques; dans â€˜RÃ©fÃ©rencesâ€™, pas dâ€™emojis Ã  lâ€™intÃ©rieur des liens euxâ€‘mÃªmes. "
    "PrÃ©serve la clartÃ© et Ã©vite toute surabondance ou emojis ambigus/infantilisants.\n\n"
    "LisibilitÃ©: sÃ©pare chaque section par une ligne horizontale '---'. Ã‰vite les tableaux et les listes procÃ©durales dÃ©taillÃ©es; privilÃ©gie des puces courtes centrÃ©es sur dÃ©cision et conduite Ã  tenir.\n\n"
    "MÃ©dicaments (FR): DCI | dose | voie | frÃ©quence | max | ajustements rÃ©nal/hÃ©patique | contreâ€‘indications | interactions clÃ©s. "
    "Appuieâ€‘toi sur RCP/ANSM; si doute: â€˜VÃ©rifier Vidal/RCP local.â€™\n\n"
    "RÃ©fÃ©rences (FR prioritaires): HAS, ANSM, SantÃ© publique France, SociÃ©tÃ©s savantes FR (ex. SFC), puis ESC/ACC/AHA si FR indisponible. "
    "Liens canoniques uniquement.\n\n"
    "SchÃ©ma de sortie (FR) (adapter et raccourcir selon pertinence):\n"
    "### Questions de clarification\n(1â€“3 puces si donnÃ©es clÃ©s manquantes)\n\n"
    "### Triage\nBadge: Urgence vitale / Urgent (<48 h) / Routine â€” 1 ligne de justification.\n\n"
    "### RÃ©sumÃ© clinique\nÃ‚ge/sex, contexte, comorbiditÃ©s, traitements/allergies, Ã©lÃ©ments saillants.\n\n"
    "### Diagnostic diffÃ©rentiel\nTop 3 â€” (haute/modÃ©rÃ©e/faible) â€” 1 ligne de rationnel chacun.\n\n"
    "### Signes dâ€™alarme\n3â€“6 puces spÃ©cifiques.\n\n"
    "### Bilan initial\nExamens immÃ©diats + justification minimale; si â€˜Xâ€™ anormal â†’ â€˜Yâ€™.\n\n"
    "### Plan de prise en charge\nNonâ€‘pharm (3 actions). Pharm (DCI | dose | voie | frÃ©quence | max | ajustements | CI/IA). Suivi (dÃ©lai, objectifs).\n\n"
    "### Points de conseil patient\n1â€“4 puces, langage clair.\n\n"
    "### Ã€ documenter\nÃ‰lÃ©ments Ã  tracer (ex: score risque, infoâ€‘consentement).\n\n"
    "### RÃ©fÃ©rences\n2â€“4 liens (HAS/ANSM/SPF/SFC; sinon ESC/ACC/AHA).\n\n"
    "Ajoute Ã  la fin: â€˜Aide Ã  la dÃ©cision â€“ ne remplace pas lâ€™avis clinique ni les rÃ©fÃ©rentiels locaux.â€™"
)


@dataclass
class AWSConfig:
    region: Optional[str] = None
    access_key_id: Optional[str] = None
    secret_access_key: Optional[str] = None
    session_token: Optional[str] = None


@dataclass
class ModelConfig:
    model_id: str = DEFAULT_MODEL_ID
    max_tokens: int = DEFAULT_MAX_TOKENS
    temperature: float = DEFAULT_TEMPERATURE
    system_prompt: str = DEFAULT_SYSTEM_PROMPT_FR
    # If set, calls will be made via this Bedrock Inference Profile instead of model_id
    inference_profile_arn: Optional[str] = None


@dataclass
class AppConfig:
    aws: AWSConfig
    model: ModelConfig


def _get_env(name: str, default: Optional[str] = None) -> Optional[str]:
    val = os.getenv(name)
    return val if val not in (None, "") else default


def _load_json(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def load_config(path: Optional[str] = None) -> AppConfig:
    """
    Load application configuration from a JSON file and environment variables.

    Precedence (highest to lowest):
      1. Environment variables
      2. JSON file at MEDAI_CONFIG_PATH or explicit `path`

    Supported env vars:
      - AWS_REGION or AWS_DEFAULT_REGION
      - AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN
      - BEDROCK_MODEL_ID, MEDAI_SYSTEM_PROMPT
      - MEDAI_MAX_TOKENS, MEDAI_TEMPERATURE
      - MEDAI_CONFIG_PATH (if `path` not provided)
    """
    cfg_path = path or _get_env("MEDAI_CONFIG_PATH")
    data: Dict[str, Any] = {}
    if cfg_path and os.path.exists(cfg_path):
        try:
            data = _load_json(cfg_path)
        except Exception:
            # Ignore parse errors and fall back to env only
            data = {}

    # Accessors from JSON structure {"aws": {...}, "model": {...}}
    aws_data = data.get("aws", {}) if isinstance(data, dict) else {}
    model_data = data.get("model", {}) if isinstance(data, dict) else {}

    aws = AWSConfig(
        region=_get_env("AWS_REGION", _get_env("AWS_DEFAULT_REGION", aws_data.get("region"))),
        access_key_id=_get_env("AWS_ACCESS_KEY_ID", aws_data.get("access_key_id")),
        secret_access_key=_get_env("AWS_SECRET_ACCESS_KEY", aws_data.get("secret_access_key")),
        session_token=_get_env("AWS_SESSION_TOKEN", aws_data.get("session_token")),
    )

    # Numeric envs
    def _int_env(name: str, default: int) -> int:
        val = os.getenv(name)
        if val is None:
            return default
        try:
            return int(val)
        except ValueError:
            return default

    def _float_env(name: str, default: float) -> float:
        val = os.getenv(name)
        if val is None:
            return default
        try:
            return float(val)
        except ValueError:
            return default

    model = ModelConfig(
        model_id=_get_env("BEDROCK_MODEL_ID", model_data.get("model_id", DEFAULT_MODEL_ID)) or DEFAULT_MODEL_ID,
        max_tokens=_int_env("MEDAI_MAX_TOKENS", int(model_data.get("max_tokens", DEFAULT_MAX_TOKENS))),
        temperature=_float_env("MEDAI_TEMPERATURE", float(model_data.get("temperature", DEFAULT_TEMPERATURE))),
        system_prompt=_get_env("MEDAI_SYSTEM_PROMPT", model_data.get("system_prompt"))
        or ModelConfig.system_prompt,
        inference_profile_arn=_get_env("BEDROCK_INFERENCE_PROFILE_ARN", model_data.get("inference_profile_arn")),
    )

    return AppConfig(aws=aws, model=model)
